%% Created for Prasanta Ghosh on -01-20

@article{Merity,
  author    = {Stephen Merity and
               Nitish Shirish Keskar and
               Richard Socher},
  title     = {Regularizing and Optimizing {LSTM} Language Models},
  journal   = {CoRR},
  volume    = {abs/1708.02182},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.02182},
  eprinttype = {arXiv},
  eprint    = {1708.02182},
  timestamp = {Mon, 13 Aug 2018 16:47:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-02182.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{PTB,
    title = {Building a Large Annotated Corpus of {E}nglish: The {P}enn {T}reebank},
    author = {Marcus, Mitchell P.  and
      Santorini, Beatrice  and
      Marcinkiewicz, Mary Ann},
    journal = {Computational Linguistics},
    volume = {19},
    number = {2},
    year = {1993},
    address = {Cambridge, MA},
    publisher = {MIT Press},
    url = {https://aclanthology.org/J93-2004},
    pages = {313--330},
}


@InProceedings{DropConnect,
  title = 	 {Regularization of Neural Networks using DropConnect},
  author = 	 {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Le Cun, Yann and Fergus, Rob},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1058--1066},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/wan13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/wan13.html},
  abstract = 	 {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.}
}


@misc{Variational,
  doi = {10.48550/ARXIV.1512.05287},
  url = {https://arxiv.org/abs/1512.05287},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  keywords = {Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A Theoretically Grounded Application of Dropout in Recurrent Neural Networks},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{ASGD,
author = {Polyak, Boris and Juditsky, Anatoli},
year = {1992},
month = {07},
pages = {838-855},
title = {Acceleration of Stochastic Approximation by Averaging},
volume = {30},
journal = {SIAM Journal on Control and Optimization},
doi = {10.1137/0330046}
}